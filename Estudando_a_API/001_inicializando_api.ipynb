{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializando a API da OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # _ é uma variável que vai guardar nada. Está dizendo ao python que vai rodar a função, mas n vai guardar.\n",
    "\n",
    "# A biblioteca python-dotenv é utilizada para gerenciar variáveis de ambiente em projetos Python. Ela permite carregar automaticamente valores de um arquivo .env para variáveis de ambiente no seu código. Isso é especialmente útil para manter informações sensíveis, como a chave da API, fora do código-fonte, evitando exposição acidental ao subir o código para repositórios públicos como o GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processo passo a passo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [{'role': 'user', 'content': 'O que é uma maça em 5 palavras?'}] # crio uma mensagem com uma pergunta.\n",
    "\n",
    "resposta = client.chat.completions.create( # mando para o Modelo responder.\n",
    "    messages=mensagens,\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    max_tokens=1000,\n",
    "    tempeture=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resposta.choices[0].message.content) # obtenho o conteúdo da mensagem respondida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens.append({'role': 'assintent', 'content': resposta.choices[0].message.content}) # adiciono na conversa a mensagem respondida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens.append({'role': 'user', 'content': 'E qual é a sua cor?'}) # crio uma mensagem com uma nova pergunta e adiciono na conversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens #vejo a conversa toda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta = client.chat.completions.create( # mando para o Modelo responder novamente!\n",
    "    messages=mensagens,\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    max_tokens=1000,\n",
    "    tempeture=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resposta.choices[0].message.content) # visualizo a mensagem respondida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando a uma função o processo de Resposta, Print() e Append()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geracao_texto(mensagens, model='gpt-3.5-turbo-0125', max_tokens=1000, temperature=0): # já entra os parâmetros requeridos\n",
    "    resposta = client.chat.completions.create(  # Usa o cliente OpenAI (instância 'client') para gerar uma resposta.\n",
    "        messages=mensagens,  # Envia as mensagens fornecidas como histórico de contexto ao modelo.\n",
    "        model=model,  # Especifica qual modelo será utilizado.\n",
    "        max_tokens=max_tokens,  # Define o número máximo de tokens (palavras ou fragmentos) gerados na resposta.\n",
    "        temperature=temperature  # Define a \"criatividade\" da resposta\n",
    "    )\n",
    "    print(resposta.choices[0].message.content)  # Imprime o conteúdo da primeira escolha de resposta gerada pelo modelo ('choices[0].message.content'). # Normalmente, o GPT pode gerar múltiplas respostas, mas aqui estamos acessando apenas a primeira!\n",
    "    mensagens.append({'role': 'assintent', 'content': resposta.choices[0].message.content}) # Adiciona a resposta do modelo à lista de mensagens como se fosse o papel ('role') do 'assistant', # com o conteúdo ('content') da resposta gerada. Isso atualiza o histórico da conversa.\n",
    "    return mensagens  # Retorna a lista de mensagens atualizada, que agora inclui a resposta gerada pelo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [{'role': 'user', 'content': 'O que é uma maça em 5 palavras?'}] # crio uma mensagem com uma pergunta.\n",
    "mensagens = geracao_texto(mensagens) # mando para o Modelo responder usando a função geracao_texto()/obtenho o conteúdo da mensagem respondida/adiciono na conversa a mensagem respondida/vejo a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens.append({'role': 'user', 'content': 'E qual é a sua cor?'}) # faço uma nova pergunta\n",
    "mensagens = geracao_texto(mensagens) # mando para o Modelo responder/obtenho o conteúdo da mensagem respondida/adiciono na conversa a mensagem respondida/vejo a resposta E ASSIM, SUCESSIVAMENTE..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando a classe de resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [{'role': 'user', 'content': 'O que é uma maçã em 5 palavras?'}]\n",
    "resposta = client.chat.completions.create(\n",
    "    messages=mensagens,\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    max_tokens=1000,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "CompletionUsage(completion_tokens=12, prompt_tokens=10, total_tokens=22)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicação Detalhada de resposta.usage:\n",
    "A propriedade .usage contém informações detalhadas sobre o uso de tokens em uma requisição à API da OpenAI. Essa informação é importante para entender a quantidade de tokens que foi consumida na geração de uma resposta, o que pode impactar custos e limites de uso da API. Vamos analisar cada parte dessa funcionalidade\n",
    "\n",
    "Após a execução da chamada, você pode acessar o atributo resposta.usage para obter um resumo do uso de tokens. Esse atributo fornece informações como:\n",
    "\n",
    "```prompt_tokens```: A quantidade de tokens que foram usados para o prompt, ou seja, a entrada que você forneceu ao modelo (mensagens). No seu exemplo, isso inclui a pergunta 'O que é uma maçã em 5 palavras?' e qualquer outro contexto anterior que possa ter sido passado. A contagem de tokens depende do tamanho do texto enviado e de como o modelo o \"quebra\" em tokens.\n",
    "\n",
    "```completion_tokens```: A quantidade de tokens que foram gerados na resposta pelo modelo. Esses tokens referem-se ao conteúdo que foi criado pelo modelo a partir do prompt e que você definiu o limite com o parâmetro max_tokens.\n",
    "\n",
    "```total_tokens```: A soma de prompt_tokens e completion_tokens. Isso representa o número total de tokens usados em toda a interação, incluindo tanto a entrada (prompt) quanto a saída (resposta). Esse valor é útil para controle de consumo, pois o número de tokens geralmente está relacionado ao custo da utilização da API.\n",
    "\n",
    "Suponha que a saída de resposta.usage seja:\n",
    "\n",
    "```python\n",
    "CompletionUsage(completion_tokens=12, prompt_tokens=10, total_tokens=22)\n",
    "```\n",
    "\n",
    "```prompt_tokens: 10```: Significa que a pergunta 'O que é uma maçã em 5 palavras?' foi \"quebrada\" em 10 tokens.\n",
    "\n",
    "```completion_tokens: 12```: A resposta gerada pelo modelo (exemplo: 'Uma fruta doce e suculenta.') utilizou 12 tokens.\n",
    "\n",
    "```total_tokens: 22```: O total de tokens usados no processo foi 22, que é a soma de prompt_tokens e completion_tokens.\n",
    "\n",
    "### Utilidade de resposta.usage:\n",
    "**Monitoramento de Custos**: Muitas vezes o custo do uso da API da OpenAI é baseado na quantidade de tokens usados. Portanto, ao monitorar o uso de tokens, você pode controlar o consumo e, consequentemente, os gastos.\n",
    "\n",
    "**Otimização de Requisições**: Analisando o número de tokens usados, é possível otimizar as chamadas para economizar tokens, por exemplo, enviando prompts mais concisos ou ajustando os limites de tokens nas respostas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta.choice[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ChatCompletionMessage(content='Fruta redonda e saborosa', role='assistent', function_call=None, tool_calls=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta.choice[0].message.model_dump() # model_dump() transforma a saída em Dicionário Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{'content': 'Fruta redonda e saborosa',\n",
    " 'role': 'assistent',\n",
    " 'function_call': None,\n",
    " 'tool_calls': None}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta.choice[0].message.model_dump(exclude_none=True) # exclude_none=True remove os parâmetros que não foram utilizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{'content': 'Fruta redonda e saborosa', 'role': 'assistent',}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar o método dump() para melhorar as respostas da função ``` geracao_texto()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geracao_texto(mensagens, model='gpt-3.5-turbo-0125', max_tokens=1000, temperature=0):\n",
    "    resposta = client.chat.completions.create( \n",
    "        messages=mensagens, \n",
    "        model=model, \n",
    "        max_tokens=max_tokens, \n",
    "        temperature=temperature \n",
    "    )\n",
    "    print(resposta.choices[0].message.content) \n",
    "    mensagens.append(resposta.choices[0].message.model_dump(exclude_none=True)) # substituindo usando model_dump()\n",
    "    return mensagens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
