{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando uma Stream de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = find_dotenv(load_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [{'role': 'user', 'content': 'crie uma história sobre uma viagem a marte'}]\n",
    "resposta = client.chat.completions.create(\n",
    "    messages=mensagens,\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    stream=True # inserido parâmetro stream. \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "#O retorno inicial de uma chamada em streaming é um objeto de stream, que funciona como um iterador.\n",
    "<openai.Stream at 0x1eb-e5ffe50> \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A opção stream=True é usada para ativar o streaming. Isso significa que a resposta não será retornada como um único bloco de texto, mas em \"pedaços\" (```chunks```), à medida que o modelo gera os tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temos que iterar e pegar cada parte do token que estão divididos por chunks\n",
    "# Eles são do tipo ChatCompletionChunk\n",
    "resposta_completa = \"\"\n",
    "for stream_resposta in resposta:\n",
    "    texto = stream_resposta.choices[0].delta.content\n",
    "    if texto:\n",
    "        resposta_completa += texto\n",
    "        print(texto, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```Iteração sobre o Objeto de Stream```:\n",
    "    - A variável resposta é um iterador, e você pode iterar sobre ele para acessar cada chunk gerado pelo modelo.\n",
    "    - O loop for percorre cada parte da resposta conforme ela vai sendo gerada.\n",
    "- ```Acessando o Conteúdo de Cada Chunk```:\n",
    "    - Dentro do loop, a resposta parcial está no campo delta.content de choices[0]. Esse campo contém a parte do texto que foi gerada até o momento.\n",
    "- ```Acumulação e Exibição do Texto```:\n",
    "    - A cada iteração, se houver conteúdo, ele é adicionado à variável resposta_completa.\n",
    "    - O print() exibe cada parte do texto sem quebrar a linha, permitindo que o conteúdo seja exibido de maneira contínua."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importância do Streaming:\n",
    "```**Baixa Latência**```: O streaming permite que você receba a resposta de maneira incremental, começando a processá-la ou exibi-la antes que o modelo tenha finalizado toda a geração.\n",
    "\n",
    "```**Uso em Aplicações em Tempo Real**```: Isso é útil em casos como chatbots, onde a experiência do usuário é aprimorada ao fornecer respostas rápidas e contínuas, sem precisar esperar por uma resposta completa.\n",
    "\n",
    "### Exemplo de Funcionamento:\n",
    "Durante a execução, o texto gerado sobre a \"história de uma viagem a Marte\" será exibido em partes. Cada fragmento gerado será impresso e acumulado em resposta_completa, formando a resposta final ao longo do tempo.\n",
    "\n",
    "### Conclusão:\n",
    "Esse método de streaming permite gerar e processar respostas de maneira eficiente e em tempo real, tornando-o ideal para cenários onde é necessária uma resposta rápida ou a exibição parcial das saídas enquanto ainda estão sendo geradas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
