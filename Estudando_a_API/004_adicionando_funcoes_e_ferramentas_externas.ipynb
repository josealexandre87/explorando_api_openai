{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**```Explicação Geral sobre o Código e Tools da API da OpenAI```** <br>\n",
    " - O código apresentado utiliza a API da OpenAI para gerar respostas baseadas em ferramentas (tools) customizadas. Nesse caso, temos uma função chamada obter_temperatura_atual, que simula a obtenção da temperatura em algumas cidades. O código permite que o modelo da OpenAI utilize essa função como uma ferramenta adicional para fornecer informações ao usuário.\n",
    "\n",
    "**```Como Funciona a Integração com Ferramentas```**<br>\n",
    " - As tools (ferramentas) são integradas ao modelo GPT para permitir que ele execute funções externas e retorne os resultados como parte de sua resposta. Nesse exemplo, o modelo decide qual ferramenta chamar com base nas mensagens do usuário e na descrição das ferramentas disponíveis. O conceito de \"tools\" amplia as capacidades do modelo, permitindo-lhe fazer cálculos, chamar APIs, ou realizar operações personalizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_temperatura_atual(local, unidade=\"celcius\"): # a função está hardcoded. Poderia ser uma API.\n",
    "    if \"são paulo\" in local.lower():\n",
    "        return json.dumps(\n",
    "        {\"local\": \"São Paulo\", \"temperatura\": \"32\", \"unidade\": unidade}\n",
    "        )\n",
    "    elif \"porto alegre\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"Porto Alegre\", \"temperatura\": \"25\", \"unidade\": unidade}\n",
    "        )\n",
    "    elif \"rio de janeiro\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"Rio de Janeiro\", \"temperatura\": \"35\", \"unidade\": unidade}\n",
    "        )\n",
    "    else:\n",
    "        return json.dumps(\n",
    "            {\"local\": local, \"temperatura\": \"unknown\"}\n",
    "        ) # json.dumps -> pega um dicionário e transforma em um json.\n",
    "#---\n",
    "tools = [ #Temos que descrever para o modelo tudo sobre as Ferramentas/Tools que ele tem para usar!\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"obter_temperatura_atual\", #importante para o modelo saber a descrição explicita da função!\n",
    "            \"description\": \"Obtém a temperatura atual em uma dada cidade\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\", # O tipo dos parâmetros é um objeto JSON\n",
    "                \"properties\": {#são os parâmetros da função! def obter_temperatura_atual(local, unidade=\"celcius\")\n",
    "                    \"local\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"O nome da cidade. Ex: São Paulo\",\n",
    "                    },\n",
    "                    \"unidade\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celcius\", \"fahrenheit\"]\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"local\"],  # O parâmetro \"local\" é obrigatório\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "#---\n",
    "funcoes_disponiveis = { # Mapeia o nome das ferramentas para a função real em Python\n",
    "    \"obter_temperatura_atual\": obter_temperatura_atual,\n",
    "}\n",
    "# Definição da mensagem inicial do usuário solicitando a temperatura em duas cidades\n",
    "mensagens = [\n",
    "    {\"role\": \"user\",\n",
    "    \"content\": \"Qual é a temperatura em São Paulo e Porto Alegre?\"}\n",
    "]\n",
    "# Chama a API da OpenAI para gerar uma resposta\n",
    "resposta = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    messages=mensagens,\n",
    "    tools=tools, # Define as ferramentas disponíveis para o modelo\n",
    "    tool_choice=\"auto\", # Permite que o modelo escolha automaticamente qual ferramenta usar\n",
    "    )\n",
    "#---\n",
    "mensagem_resp = resposta.choices[0].message # Obtém a resposta inicial do modelo\n",
    "tool_calls = mensagem_resp.tool_calls  # Verifica se a resposta inclui chamadas de ferramentas\n",
    "\n",
    "if tool_calls: # Se houver chamadas de ferramentas, processa cada uma delas\n",
    "    mensagens.append(mensagem_resp) # Adiciona a resposta inicial à lista de mensagens\n",
    "    for tool_call in tool_calls:  # Itera sobre cada chamada de ferramenta\n",
    "        function_name = tool_call.function.name  # Obtém o nome da função chamada\n",
    "        function_to_call = funcoes_disponiveis[function_name]  # Busca a função correspondente no dicionário de funções\n",
    "        function_args = json.loads(tool_call.function.arguments)  # Extrai os argumentos da função em formato JSON\n",
    "        function_response = function_to_call(  # Chama a função real passando os argumentos extraídos\n",
    "            local=function_args.get(\"local\"),\n",
    "            unidade=function_args.get(\"unidade\"),\n",
    "        )\n",
    "        mensagens.append( # Adiciona a resposta da ferramenta à lista de mensagens\n",
    "            {\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\", # A função agindo como uma ferramenta\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,  # Resposta da função (temperatura da cidade)\n",
    "            }\n",
    "        )\n",
    "# Gera uma nova resposta final com base nas mensagens atualizadas (incluindo a resposta da ferramenta)\n",
    "    segunda_resposta = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=mensagens,\n",
    "    )\n",
    "# Exibe a resposta final gerada pelo modelo com as informações da ferramenta\n",
    "mensagens_resp = segunda_resposta.choices[0].message\n",
    "print(mensagem_resp.content) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**```Explicação por Blocos```**\n",
    "\n",
    "**1. Função ```obter_temperatura_atual```:**\n",
    "\n",
    " - Essa função simula a obtenção da temperatura de cidades específicas (São Paulo, Porto Alegre, Rio de Janeiro). A função utiliza ``json.dumps`` para retornar os dados no formato JSON.\n",
    " - Se o local não for uma dessas cidades, o valor da temperatura será ``\"unknown\"``, indicando que a cidade não é reconhecida.\n",
    "\n",
    "**2. Definição de Ferramentas:**\n",
    " - Um dicionário com o nome da função e sua descrição detalhada é definido para o modelo, permitindo que ele use essa função quando necessário. O parâmetro obrigatório ``local`` e o opcional ``unidade`` são especificados. Isso orienta o modelo sobre como chamar a função corretamente.\n",
    "\n",
    "**3. Execução do Modelo com Ferramentas:**\n",
    "\n",
    " - O código faz uma chamada para o modelo GPT com uma mensagem do usuário, e permite que o modelo utilize as ferramentas definidas para gerar uma resposta. A chamada de ferramentas é automática com o parâmetro ``\"tool_choice\": \"auto\"``.\n",
    " - O modelo escolhe qual ferramenta usar com base na descrição da função e nas mensagens do usuário.\n",
    "\n",
    "**4. Processamento das Chamadas de Ferramentas:**\n",
    "\n",
    " - Caso o modelo decida utilizar uma ferramenta, ele retorna os detalhes da chamada. O código então processa essas chamadas, invocando a função Python correspondente e passando os parâmetros.\n",
    " - A resposta da função (``obter_temperatura_atual``) é adicionada ao fluxo de mensagens e enviada de volta ao modelo para gerar uma resposta final.\n",
    "  \n",
    "**5. Resposta Final:**\n",
    "\n",
    " - Após o uso das ferramentas, o modelo gera uma resposta final, incluindo os resultados das funções chamadas. Essa resposta é exibida ao usuário.\n",
    "---\n",
    "Esse código demonstra como integrar funções personalizadas como ferramentas dentro da API da OpenAI. O modelo pode chamar funções para realizar tarefas específicas, como calcular ou obter informações, e usa essas respostas para gerar uma interação mais rica com o usuário."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
